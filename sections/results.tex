\section{Results}
\subsection{Motivating Example: Improving a Standard ImageNet Baseline}
\label{subsec:imagenet-baseline}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/14.00-ehv-motivating-plot.png}
    \caption{Trying to improve a standard ImageNet baseline. Each panel shows the change in performance achieved by swapping out one component of a standard ImageNet baseline (No Augmentation, ResNet-50, BCE Loss, Standard Pooling) with a different design choice. Remarkably there is \emph{no single change we can make} that results in a statisticaly significant improvement in AUROC across all three benchmarks. }
    \label{fig:imagenet-baseline}
\end{figure}
We begin our evaluation of design choices in chest x-ray classification by considering a simple task: improving a standard ImageNet model. A ResNet-50 trained via BCE loss without special data augmentation or pooling is a standard baseline in general image classification \citep{developers2016pytorch}, however given the development of domain-specific methods and practices for chest x-ray classification we might expect that we can improve on this baseline substantially through changes in each of its components. Figure \ref{fig:imagenet-baseline} shows how performance changes when we consider changing the data augmentation, pooling method, backbone, and loss function for this simple baseline model. Each row shows the difference in performance between the baseline model and a model which uses a different choice for one of these components, keeping all the other components fixed. 

Remarkably, there is \emph{no change we can make in a single component} which significantly improves performance across all three chest x-ray datasets. Moreover it is notable that one domain-specific choice--hierarchical training--actually consistently performs \emph{worse} than this simple baseline. This example suggests that there may be a need for more careful evaluation of the performance benefits given by design choices used in chest x-ray classification.

\subsection{Comparing Design Choices}
\label{subsec:controlled-comparison}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/12.08-ehv-controlled-comparison.png}
    \caption{Average Treatment Effect of design choices over baseline choices. We conduct a controlled comparison of design choices by evaluating the average change in AUROC across all methods when we swap out a baseline choice. Almost no design choices consistently increase or decrease AUROC across all three datasets. Those that do (PCAM pooling, CheXNet loss type, ResNet-18 and VGG-16 backbone, and CIFAR data augmentation) produce changes that are often less than 0.5\% AUROC meaning they have a very small treatment effect. }
    \label{fig:controlled-comparison}
\end{figure}
Motivated by our results in Section \ref{subsec:imagenet-baseline}, we aim to conduct a more systemic evaluation of how common design choices improve over standard image classification baselines. Because we train a model for every choice of (data augmentation, pooling, backbone, loss type) we are able to evaluate the difference in AUROC between a model that uses a baseline choice for one of these components and a model that uses a different choice for that component \emph{leaving all other choices fixed}. This gives us a \emph{treatment effect} of that design choice over the baseline choice. We can then compute an \emph{average treatment effect} by averaging this treatment effect over all choices for the other components. We do exactly this in Figure \ref{fig:controlled-comparison}. Our baseline choices are the standard setup described in the last section: No Augmentation, Standard Pooling, ResNet-50, and BCE Loss. In each row we see the average treatment effect of different choices bootstrapped over 1000 samples of the test dataset for each benchmark.

The most striking result from this figure is that almost no design choices have a statistically significant effect on AUROC across all three benchmarks. Among loss functions, only CheXNet loss has a consistently statistically significant effect on AUROC, and even then the effect is small (< 0.5\% AUROC). Furthermore it is worth noting CheXNet loss is the least domain-specific of the domain-specific loss functions we consider, effectively just a weighted version of BCE loss. A similiar story holds for the other choices that have a statistically significant effect on AUROC across all three benchmarks. The only exception is VGG-16, but this is because it seems to be a particularly poor choice of architecture for chest x-ray classification, not because it leads to significant improvements. Especially notable is the fact that the DenseNet-121 architecture, which has been noted to be a strong choice for chest x-ray classification in the dicta of many past works \citep{irvin2019chexpert,pham2021interpreting,rajpurkar2017chexnet}, provides essentially no treatment effect. 

That design choices designed for the chest x-ray domain such as Hierarchical Loss, DAM, and PCAM produce such small treatment effects is especially surprising. When engineered to produce peak performance, these methods have matched human radiologists on some tasks \citep{irvin2019chexpert}. However, our results suggest that these methods in isolation are not significantly better than a standard image classification baseline.

\subsection{Comparing domain-specific choices to baselines ones}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/06.04-ehv-best-method-comparison-remake.png}
    \caption{
        Comparing the best versions of domain-specific design choices with domain-independent baselines. For each domain-specific design choice we compare the best method we can construct using that choice with the best method we can construct using only domain-independent choices. We find that on CheXpert and MIMIC the domain-independent baseline is just as strong as the best methods we can construct using domain-specific choices. On Chest X-ray 14 the domain-independent baseline is not quite as strong as the best methods using domain-specific choices, but it is still comparable to Hierarchical Loss and it is within 1\% AUROC of other methods.
    }
    \label{fig:best-method-per-choice}
\end{figure}
Results from Section \ref{subsec:controlled-comparison} suggest that domain-specific design choices may not have a significant effect on AUROC. However in that section we computed the performance of domain-specific design choices across all configurations of design choices. In order to fairly evaluate these design choices it is also important to compare the \emph{best} performance we can achieve using domain-specific design choices with the performance of domain-independent baselines. We do exactly this in Figure \ref{fig:best-method-per-choice}.

For each domain-specific design choice we compare the best method we can construct using that choice with the best method we can construct using only domain-independent choices (Baseline). In order to evaluate the source of improvement of a domain-specific choice over a baseline, we also include the best models we can construct using domain-independent choices for the same component but allowing domain-specific choices for the other components (e.g. the best model we can make that uses Focal Loss while also allowing the use of PCAM). 

On CheXpert and MIMIC, we find that suprisingly even the best methods we can construct using domain-specific design choices are not significantly better than the best methods we can construct using only domain-independent choices. Furthermore, although on Chest X-ray 14 the baseline is not quite as strong as the domain-specific methods, when we allow for domain-specific choices in other components we see that domain-independent choices are comparable in performance. These results suggest that we have yet to determine how best to leverage domain-specific knowledge in chest x-ray classification to improve performance over baselines.

% \subsection{Comparing Attribute Choices}
% The most straightforward way to compare the performance of domain specific attribute choices to domain independent choices is to consider the best methods we can construct holding each of these choices fixed. Because we have chest x-ray specific choices for both pooling function and loss function, it is also useful to construct a set of \emph{baseline} methods which use only domain independent choices for these attributes. We show the best choices for these baseline methods alongside the best methods we can construct holding each attribute choice fixed for each dataset in Figure \ref{fig:best-method-per-attribute}.

% On CheXpert and MIMIC one striking conclusion we can draw is that the baseline method is just as strong as the best methods we can construct even when using domain-specific attributes. This is surprising and suggests that existing chest x-ray specific methods may not be meaningfully better than domain independent baselines on these datasets.

% On Chest X-ray 14 we see a similiar story. Although the pure baseline method is not quite as strong when compared to the best methods using domain specific attributes, it is still comparable to Hierarchical Loss and it is within 1\% AUROC of other methods. Furthermore our setup allows us to evaluate what the cause of increased performance is for these methods.

% The only difference between the best baseline method and the best method using Focal Loss is the use of PCAM.  Comparing the best method for Focal Loss with the best methods for domain-specific loss functions, we see that once the use of PCAM is controlled for, domain-independent loss functions are just as strong as domain-specific loss functions. 

% We get the same result when considering the best methods for PCAM or Standard pooling. The only difference between the best method for Standard pooling and the best method for PCAM is the use of DAM Loss. Once this is controlled for, we see that PCAM and Standard pooling are indistinguishable in terms of performance.

% These results suggest that there is still room to improve chest x-ray classification performance by developing new domain specific methods. However, they also suggest that the current state of the art methods which are domain specific may not be as strong as they appear when compared to domain independent baselines.

% Lastly it is worth noting that Figure \ref{fig:best-method-per-attribute} also lets us analyze choices for other attributes such as Backbone and Data Augmentation. In contrast to prior work \citep{irvin2019chexpert,pham2021interpreting} we find little difference between backbones on CheXpert and MIMIC and suprisingly find VGG-19 to be one of the best backbones on Chest X-ray 14. We also find that data augmentation only seems to matter on Chest X-ray 14. While these results are not the primary focus of our study, the contrast with prior work suggests that the best choices for these attributes may be dependent on the other choices made in the method being considered and that additional work may be needed to understand the best choices for these attributes.

% \begin{figure}
%     % Make three figures in the following arrangment
%     %  A  B C
%     \centering
%     \includegraphics[scale=0.6]{figures/06.03-ehv-bootstrap-mega-figure.png}
%     \caption{\textbf{Comparison of Best Method Per Attribute}. For each dataset we take the best method for each choice of the attribute considered and show the test AUROC with confidence intervals. The baseline is restricted to only domain independent attribute choices. On CheXpert and MIMIC we see that the baseline is just as strong as the best methods even for domain-specific attributes like Hierarchical, DAM, and PCAM. On Chest X-ray 14 while the baseline alone is not quite as strong, when using PCAM domain independent loss functions are just as strong as domain-specific loss functions, and when using domain specific loss functions standard pooling is just as strong as PCAM.} 
%     \label{fig:best-method-per-attribute}
% \end{figure}

% \begin{figure}
%         \centering
%     \includegraphics[scale=0.6]{figures/09.02-ensemble-comparison.png}
%     \caption{\textbf{Comparison of Best Methods Ensembled}. For each dataset we consider an ensemble of the best methods we can construct holding each of our Loss and Pooling attribute choices fixed as well as an ensemble of our best baseline methods. We also show the results from Figure \ref{fig:best-method-per-attribute} using transparent colors for comparison. We see similiar conclusions as in Figure \ref{fig:best-method-per-attribute}: on CheXpert and MIMIC the baseline ensemble is just as strong as domain-specific ensembles. It is also worth noting that ensembling provides a consistent boost in performance for all methods, in line with previous work.} 
%     \label{fig:best-method-per-ensemble}

% \end{figure}
% \subsection{Ensembling}
% Ensembling of diverse methods is a common approach in chest x-ray classification and many of the best results have been reported based on ensembles of multiple methods holding on specific attribute (such as a loss function) fixed \citep{pham2021interpreting,yuan2020large,ye2020weakly}. Thus it is important to see whether the conclusion we draw from the best individual methods change when we consider an ensemble of methods.

% In Figure \ref{fig:best-method-per-ensemble} we show the results of ensembling the best methods we can construct holding each attribute choice fixed as well as ensembling the best baseline methods. We see that the conclusions we draw from Figure \ref{fig:best-method-per-attribute} hold even when ensembling is used. On CheXpert and MIMIC the baseline ensemble is just as strong as domain-specific ensembles. 

% On Chest X-ray 14 one important conclusion we can draw is that an ensemble of baseline methods is just as strong as the best individual methods which use domain-specific attributes. This shows that we can match the performance of the best domain-specific methods by simply ensembling the best domain-independent methods.

% We also see that for Chest X-ray14 ensembles for domain independent loss functions are just as strong as the ensembles for domain specific loss functions when we control for whether PCAM use is allowed. Similiarly we see that the ensembles for PCAM and Standard pooling are indistinguishable when we control for whether DAM use is allowed.


% Finally, comparing the ensembles with the best individual version of each method, we see that in line with previous work, ensembling provides a consistent performance boosts for all methods. This suggests that while existing domain-specific methods for chest x-ray classification may not always provide a significant boost over domain-independent baselines, ensembling remains a promising way to boost performance. 

% \begin{figure}[]
%     % Make three figures in the following arrangment
%     %  A  B C
%         \includegraphics[width=\textwidth]{figures/12.04-ehv-loss-type-differences.png}
%     \caption{\textbf{Controlled Differences Between Loss Functions}. For each choice of (backbone, pooling type, data augmentation) we consider the difference between using a domain specific loss function (Hierarchical, DAM, CheXNet) and a domain independent loss function (Focal). We then compute the distribution of differences across all both choices of (backbone, pooling type, data augmentation) and bootstrapped samples of the test dataset. For each dataset we show the median difference with 95\% confidence intervals. In all cases we cannot conclude that the domain specific loss functions are better than the domain independent loss functions.}
%     \label{fig:controlled-loss}
% \end{figure}

% \begin{figure}[]
%     % Make three figures in the following arrangment
%     %  A  B C
%         \includegraphics[width=\textwidth]{figures/12.04-ehv-pcam-differences.png}
%     \caption{\textbf{Controlled Differences Between Pooling Types}. For each choice of (loss function, backbone, data augmentation) we consider the difference between using PCAM and Standard pooling. We then compute the distribution of differences across all both choices of (loss function, backbone, data augmentation) and bootstrapped samples of the test dataset. For each dataset we show the median difference with 95\% confidence intervals. In all cases we cannot conclude that PCAM is significantly better than Standard pooling.}

%     \label{fig:controlled-pcam}
% \end{figure}

% \subsection{Controlled Evaluation}
% The previous two analyses have shown that when we consider the best methods we can construct holding each attribute choice fixed, domain independent choices are just as strong as domain specific choices. However this is not all we can explore with our setup. Because we create methods through a grid of attribute choices, for every method we can examine what would happen if we changed just one of the attribute choices from domain independent to domain dependent. This effectively allows us to compute the treatment effect of each attribute choice on performance.

% As in our previous two analyses, we must remember that measurements of performance difference on the test set are only estimates of the difference in performance on the population. We can again use the bootstrap to estimate the uncertainty in our estimates. For each method, we first sample a new test set and then estimate the difference in performance between the domain independent and domain specific versions of the attribute choice. We repeat this process 1000 times for each method to get a distribution of performance differences. We then compute the median difference and 95\% confidence intervals for each dataset.

% We use this procedure to estimate the difference in performance between domain independent and domain specific loss functions in Figure \ref{fig:controlled-loss}. We use Focal Loss as our domain independent loss choice and compare it to the domain dependent loss functions (Hierarchical, DAM, CheXNet) for each dataset. On all three datasets and for all three loss functions, we see that their improvements over focal loss are not statistically different from zero. This shows that even in a controlled setting, we cannot conclude that domain specific loss functions are better than domain independent loss functions.

% We use the same procedure to estimate the difference in performance between PCAM and Standard pooling in Figure \ref{fig:controlled-pcam}. Again on all three datasets, the difference in performance between PCAM and Standard pooling is not statistically different from zero.

% These results are consistent with our previous analyses and suggest that their is still room to improve chest x-ray classification performance through domain specific methods.


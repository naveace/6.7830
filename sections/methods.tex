\section{Developing Intuition for the Generative Model}
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{figures/dataspace-latentspace.png}
    \caption{Developing inuition for the generative model. The example allows the user to step through the process of sampling from the generative model for PPCA. At each step a new point is sampled from latent space, modifying the lower histogram in red. The dataspace then also updates showing where that point projects to as a result of the principal direction $W$ and the noise $\sigma^2$. The user can easily change these parameters and remake the visualization to see how the data space changes.}
    \label{fig:dataspace-latentspace}
\end{figure}
Our tutorial begins with an interactive visualization of the data generating process that PPCA encodes. This visualization is shown in Figure \ref{fig:dataspace-latentspace}. The visualization consists of two main plots: data space and latent space. The user can use a slider to walk through the process of sampling from the data generating process for PPCA. At each step a new $z$ is drawn, changing the histogram in the latent space plot. A new point is then placed into data space by projecting $z$ using $W$ and randomly translating using gaussian noise given by $\sigma^2$. The user can easily modify code (not shown) to change $W$ and $\sigma^2$ to see how the process changes. The tutorial also poses some questions to the user to help them test their intuition about the data generating process. 
\section{Explaining the E-M Algorithm}
% Modify the above example to make one figure on top of two figures side by side
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{figures/em-algo-dataspace.png}
    \includegraphics[width=0.45\textwidth]{figures/em-algo-high-dim-params.png}
    \includegraphics[width=0.45\textwidth]{figures/em-algo-high-dim-elbo.png}
    \caption{Interactive examples demonstrating the fast convergence of the E-M algorithm. \textbf{(Top)} An example of using the E-M to learn $W$ with two dimensional data. Because the data is low-dimensional, we can visualize the convergence of $\hat{W}$ to the MLE estimate which is very close to the true $W$ given the amount of data. The user can step through the algorithm with a slider and use the code to change the visualization and get an intuition for how convergence changes with changes in the model parameters. \textbf{(Bottom)} A demonstration of convergence in high dimension. The plots are interactive and the user can modify the parameters of the model and the E-M algorithm to see how quickly it converges and recovers the true values.}
    \label{fig:em}
\end{figure}
While understanding the data generating process is important, the model is not very complex and as a result the primary use of PPCA is to fit the parameters to some existing data. Although closed form solutions exist for the MLE estimates of the parameters, they are not always practical to implement as they involve SVD on a $D\times D$ covariance matrix. When $D$ is large, this makes the algorithm unusable. Maintaining our focus on practical application and intuition development, the second part of our tutorial focuses on the much more efficient E-M algorithm for PPCA. Figure \ref{fig:em} shows the three interactive examples we developed for this section. The top figure shows a data space example where the user can use a slider to move through the steps of E-M and see how $\hat{W}$ converges towards the true $W$. This example is great for developing intuition as the low dimensionality of our data allows us to visualize the convergence in a very direct way. However for high dimensional data we cannot do this. Instead in the bottom figures we show how distance between the estimated and true parameters changes as a function of E-M iteration as well as how the ELBO increases. As in the previous section, the user can easily modify just a few lines in the code to change the dimensionality and true parameters to get intuition for how convergence changes as these values vary. 
\section{A Worked Example on MNIST}
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.4]{figures/mnist-example.png}
    \caption{A worked example on MNIST. We fit a PPCA model using only a handful of components to the data for one class in the MNIST dataset. Because the model is fully probabilistic, we can compute marginal likelihood over individual datapoints and see highly unlikely datapoints. This example shows the user an interesting and practical application of PPCA that would not be feasible with PCA!}
    \label{fig:mnist}
\end{figure}
The final section of our tutorial is a worked example demonstrating how PPCA allows for additional applications beyond what would be possible using standard PCA. For our example we consider the MNIST dataset--a real world dataset that is not at all necessarily generated exactly from the data generating process PPCA defines. We fit a PPCA model to the images for one digit in the dataset. We then show how we can use the fact that PPCA gives a marginal likelihood to every datapoint it fits to identify possibly mislabeled datapoints or just hard subpopulations. Figure \ref{fig:mnist} shows that this technique successfully identifies at least one impossible or mislabeled example (top left) as well as a very challenging subpopulation (cursive 2's). The user can easily modify the parameters for PPCA as well as which digit's images are used to fit the model and get a sense for how the results change. 
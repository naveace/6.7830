\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\citation{bishop2006pattern}
\citation{tipping1999probabilistic}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background on PPCA}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Developing Intuition for the Generative Model}{1}{section.3}\protected@file@percent }
\citation{tipping1999mixtures}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Developing inuition for the generative model. The example allows the user to step through the process of sampling from the generative model for PPCA. At each step a new point is sampled from latent space, modifying the lower histogram in red. The dataspace then also updates showing where that point projects to as a result of the principal direction $W$ and the noise $\sigma ^2$. The user can easily change these parameters and remake the visualization to see how the data space changes.\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dataspace-latentspace}{{1}{2}{Developing inuition for the generative model. The example allows the user to step through the process of sampling from the generative model for PPCA. At each step a new point is sampled from latent space, modifying the lower histogram in red. The dataspace then also updates showing where that point projects to as a result of the principal direction $W$ and the noise $\sigma ^2$. The user can easily change these parameters and remake the visualization to see how the data space changes.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Explaining the E-M Algorithm}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}A Worked Example on MNIST}{2}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Interactive examples demonstrating the fast convergence of the E-M algorithm. \textbf  {(Top)} An example of using the E-M to learn $W$ with two dimensional data. Because the data is low-dimensional, we can visualize the convergence of $\hat  {W}$ to the MLE estimate which is very close to the true $W$ given the amount of data. The user can step through the algorithm with a slider and use the code to change the visualization and get an intuition for how convergence changes with changes in the model parameters. \textbf  {(Bottom)} A demonstration of convergence in high dimension. The plots are interactive and the user can modify the parameters of the model and the E-M algorithm to see how quickly it converges and recovers the true values.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:em}{{2}{3}{Interactive examples demonstrating the fast convergence of the E-M algorithm. \textbf {(Top)} An example of using the E-M to learn $W$ with two dimensional data. Because the data is low-dimensional, we can visualize the convergence of $\hat {W}$ to the MLE estimate which is very close to the true $W$ given the amount of data. The user can step through the algorithm with a slider and use the code to change the visualization and get an intuition for how convergence changes with changes in the model parameters. \textbf {(Bottom)} A demonstration of convergence in high dimension. The plots are interactive and the user can modify the parameters of the model and the E-M algorithm to see how quickly it converges and recovers the true values.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A worked example on MNIST. We fit a PPCA model using only a handful of components to the data for one class in the MNIST dataset. Because the model is fully probabilistic, we can compute marginal likelihood over individual datapoints and see highly unlikely datapoints. This example shows the user an interesting and practical application of PPCA that would not be feasible with PCA!\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:mnist}{{3}{3}{A worked example on MNIST. We fit a PPCA model using only a handful of components to the data for one class in the MNIST dataset. Because the model is fully probabilistic, we can compute marginal likelihood over individual datapoints and see highly unlikely datapoints. This example shows the user an interesting and practical application of PPCA that would not be feasible with PCA!\relax }{figure.caption.4}{}}
\bibdata{bibliography/fmt_bib}
\bibcite{bishop2006pattern}{{1}{2006}{{Bishop}}{{}}}
\bibcite{tipping1999probabilistic}{{2}{1999{a}}{{Tipping and Bishop}}{{}}}
\bibcite{tipping1999mixtures}{{3}{1999{b}}{{Tipping and Bishop}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion \& Next Steps}{4}{section.6}\protected@file@percent }
\gdef \@abspage@last{4}
